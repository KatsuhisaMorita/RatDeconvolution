{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root=\"C:/github/RatDeconvolution\"\n",
    "path_package=\"C:/github/enan\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation of rat gene ontology data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filein = f\"{root}/data/rgd.gaf\" # from the web site: www.geneontology.org\n",
    "datafile = f'{root}/data/220801_go.owl' # from the web site: www.geneontology.org\n",
    "depth=5 # gene ontology depth for enrichment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import deque, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import owlready2\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OntologyStructure:\n",
    "    def __init__(self, datafile, root_term:str):\n",
    "        \"\"\"\n",
    "        constructor\n",
    "        datafile : str, the url of ontology file\n",
    "        \"\"\"\n",
    "        \n",
    "        # load datafile\n",
    "        self.onto = owlready2.get_ontology(datafile).load() # load ontology file (DL from https://www.ebi.ac.uk/efo/)\n",
    "        print(datafile,\"was loaded.\")\n",
    "        \n",
    "        # determine the roots\n",
    "        roots = []\n",
    "        for efo in self.onto.classes(): # EFO0000001 is the root\n",
    "            if root_term in str(efo):\n",
    "                roots.append(efo)\n",
    "        if len(roots) == 0:\n",
    "            print('the designated root term couldnt be found')\n",
    "            for efo in self.onto.classed():\n",
    "                if 'EFO0000001' in str(efo):\n",
    "                    roots.append(efo)\n",
    "\n",
    "        # select the first root CAUTION : this must be changed if you use an ontology structure which have multiple roots\n",
    "        self.root = roots[0] \n",
    "        print(self.root, \"was determined as root.\")\n",
    "        # with BFS, determine the depth of each GO\n",
    "        self.min_d = self.__BFS() # 幅優先探索をやってる\n",
    "        print(\"Graph structure was obtained.\")\n",
    "        \n",
    "        self.string_hash = self.__create_string_hash()\n",
    "        print(\"String hashtable was created.\")\n",
    " \n",
    "    def __BFS(self):\n",
    "        \"\"\"\n",
    "        breadth-first search function\n",
    "        \n",
    "        the function to determine the depth of each term\n",
    "        \"\"\"\n",
    "        Q = deque([self.root])\n",
    "        inf_value = lambda : float('inf')\n",
    "        min_d = defaultdict(inf_value) # defaultdictでエラーしたらinfを返すようにしている. つまり届かないところ\n",
    "        min_d[self.root] = 0 # rootの深さは0と定義\n",
    "        while len(Q)>0:\n",
    "            v = Q.pop()\n",
    "            for v1 in self.onto.get_children_of(v): # vの子v1をループ\n",
    "                if min_d[v1]>10**27: # infより大きい場合. Noneでもいいような. 要するにまだmin_dが定義されていないvの子供が現れた時の判定\n",
    "                    min_d[v1] = min_d[v] + 1 # min_dがない子供が現れたらvの深さに1段階加えて更新する\n",
    "                    Q.appendleft(v1) # Qは探索すべきものなので, 新たな階層の探索のために追加する\n",
    "        return min_d # 木構造を返す: タームを入れるとその深さを返すもの\n",
    "\n",
    "    def __create_string_hash(self):\n",
    "        \"\"\"\n",
    "        string hash function\n",
    "        \n",
    "        create the hashtable to get the class instance corresponding to the given string\n",
    "        \"\"\"\n",
    "        \n",
    "        string_hash = defaultdict(str)\n",
    "        for key in self.min_d:\n",
    "            string_hash[str(key)] = key\n",
    "        return string_hash\n",
    "\n",
    "    def get_num(self, depth):\n",
    "        \"\"\"\n",
    "        getting the number of ontology term in a specific level\n",
    "        depth : int\n",
    "        \"\"\"\n",
    "        count = 0\n",
    "        for key in self.min_d:\n",
    "            if self.min_d[key]==depth:\n",
    "                count+=1\n",
    "        return count\n",
    "    \n",
    "    def get_term(self, depth):\n",
    "        \"\"\"\n",
    "        getting the term in the specific level\n",
    "        depth :  int\n",
    "        \"\"\"\n",
    "        res = []\n",
    "        term = []\n",
    "        for key in self.min_d:\n",
    "            if self.min_d[key]==depth:\n",
    "                res.append(str(key))\n",
    "                term.append(str(key.label[0]))\n",
    "        return res,term\n",
    "    \n",
    "    def get_downstream_ID(self):\n",
    "        max_depth = max(list(self.min_d.values()))\n",
    "        print('max depth :',max_depth)\n",
    "        total_res = []\n",
    "        total_term = []\n",
    "        for i in range(1,max_depth+1):\n",
    "            tmp_res = []\n",
    "            tmp_term = []\n",
    "            for key in self.min_d:\n",
    "                if self.min_d[key]==i:\n",
    "                    tmp_res.append(str(key))\n",
    "                    tmp_term.append(str(key.label[0]))\n",
    "            \n",
    "            for t in tmp_res:\n",
    "                total_res.append(t)\n",
    "            for s in tmp_term:\n",
    "                total_term.append(s)\n",
    "        return total_res, total_term\n",
    "    \n",
    "    def get_upstream(self, go, depth):\n",
    "        \"\"\"\n",
    "        getting the upstream GO in a specific level\n",
    "        \n",
    "        go : GO class instance\n",
    "        depth : int\n",
    "        \"\"\"\n",
    "        if self.min_d[go]<2:\n",
    "            return set()\n",
    "        if self.min_d[go]==2:\n",
    "            return set([go])\n",
    "        \n",
    "        up = set()\n",
    "\n",
    "        Q = deque([go])\n",
    "        while len(Q)>0:\n",
    "            v = Q.pop()\n",
    "            try:\n",
    "                for v1 in self.onto.get_parents_of(v):\n",
    "                    if self.min_d[v1]==depth:\n",
    "                        up.add(v1)\n",
    "                    else:\n",
    "                        Q.appendleft(v1)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        return up\n",
    "    \n",
    "    def str2go(self, string):\n",
    "        \"\"\"\n",
    "        str to GO class instance\n",
    "        \"\"\"\n",
    "        return self.string_hash[string]\n",
    "\n",
    "    def go2str(self, go):\n",
    "        \"\"\"\n",
    "        GO class instance to str\n",
    "        \"\"\"\n",
    "        return str(go)\n",
    "\n",
    "    def get_depth(self, go):\n",
    "        \"\"\"\n",
    "        get the depth of GO\n",
    "        \"\"\"\n",
    "        return self.min_d[go]\n",
    "        \n",
    "    def get_term(self, depth):\n",
    "        \"\"\"\n",
    "        getting the term in the specific level\n",
    "        depth :  int\n",
    "        \"\"\"\n",
    "        res = []\n",
    "        term = []\n",
    "        for key in self.min_d:\n",
    "            if self.min_d[key]==depth:\n",
    "                res.append(str(key))\n",
    "                term.append(str(key.label[0]))\n",
    "        return res,term\n",
    "    \n",
    "    def get_downstream(self, go):\n",
    "        \"\"\"\n",
    "        getting the deepest downstream go terms\n",
    "        go : GO class instancce\n",
    "        \"\"\"\n",
    "        res = []\n",
    "        Q = deque(self.onto.get_children_of(go))\n",
    "        if len(Q)==0:\n",
    "            return [go]\n",
    "        \n",
    "        while len(Q)>0:\n",
    "            v = Q.pop()\n",
    "            res.append(v)\n",
    "            v_child = self.onto.get_children_of(v)\n",
    "            if len(v_child)==0:\n",
    "                pass\n",
    "            else:\n",
    "                for v1 in v_child:\n",
    "                    Q.appendleft(v1)\n",
    "        res = list(set(res))\n",
    "        return res     \n",
    "\n",
    "def load_go(gene_dict=dict()):\n",
    "    terms = []\n",
    "    members = []\n",
    "    for key in gene_dict.keys():\n",
    "        terms.append(key)\n",
    "        members.append(gene_dict[key])\n",
    "    return terms, members\n",
    "\n",
    "def create_dict(datafile='', gene_dict=dict(), depth:int=4):\n",
    "    # load go.owl file\n",
    "    dat = OntologyStructure(datafile, 'obo.GO_0008150') #GO_0008150 : Biological Process\n",
    "    res, term = dat.get_term(depth)\n",
    "    print('Indicated depth GO terms : {}'.format(len(res)))\n",
    "    corresp_res = [dat.get_downstream(dat.str2go(i)) for i in tqdm(res)]\n",
    "    corresp_res = [[str(v).replace('obo.GO_', 'GO:') for v in i] for i in corresp_res]\n",
    "\n",
    "    # load gene file\n",
    "    terms, members = load_go(gene_dict=gene_dict)\n",
    "    #terms = [i.split('(')[1].split(')')[0] for i in terms]\n",
    "    go_dict = dict(zip(terms, members))\n",
    "    \n",
    "    # create dict\n",
    "    res_dict=dict()\n",
    "    goperterm=[]\n",
    "    for root, i, t in zip(res,corresp_res, term):\n",
    "        res_temp = set()\n",
    "        for v in i:\n",
    "            genes_temp=go_dict.get(v, 'no')\n",
    "            if genes_temp=='no':\n",
    "                pass\n",
    "            else:\n",
    "                res_temp = res_temp or genes_temp\n",
    "        if len(res_temp)>0:\n",
    "            res_dict[root.replace('obo.GO_','GO:')+'_'+t]=res_temp\n",
    "            goperterm.append(len(res_temp))\n",
    "\n",
    "    print('No. of GO terms : {}'.format(len(res_dict)))\n",
    "    print('No. of genes / term : {}'.format(np.mean(goperterm)))\n",
    "\n",
    "    return res_dict\n",
    "\n",
    "def extract(sentence):\n",
    "    res = []\n",
    "    temp1 = sentence.split(\"RGD\\t\")\n",
    "    flag=False\n",
    "    for i in temp1:\n",
    "        temp2 = i.split(\"\\t\")\n",
    "        if len(temp2)>5:\n",
    "            symbol = temp2[1]\n",
    "            term = temp2[3]\n",
    "            if \"GO:\" in term:\n",
    "                res.append([symbol, term])\n",
    "                flag=True\n",
    "    temp1 = sentence.split(\"UniProtKB\\t\")\n",
    "    for i in temp1:\n",
    "        temp2 = i.split(\"\\t\")\n",
    "        if len(temp2)>5:\n",
    "            symbol = temp2[1]\n",
    "            term = temp2[3]\n",
    "            if \"GO:\" in term:\n",
    "                res.append([symbol, term])\n",
    "                flag=True\n",
    "    if flag:\n",
    "        return res, []\n",
    "    else:\n",
    "        return res, [sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filein, encoding='utf-8', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    res = [cols for cols in reader]\n",
    "\n",
    "res = res[39:]\n",
    "res_open=[]\n",
    "res_error1=[]\n",
    "res_error2=[]\n",
    "for i in res:\n",
    "    if type(i)==str:\n",
    "        res_temp, error = extract(i)\n",
    "        res_open += res_temp\n",
    "        res_error1 += error\n",
    "    elif len(i)==1:\n",
    "        res_temp, error = extract(i[0])\n",
    "        res_open += res_temp\n",
    "        res_error1 += error\n",
    "    else:\n",
    "        for v in i:\n",
    "            if type(v)==str:\n",
    "                res_temp, error = extract(v)\n",
    "                res_open += res_temp\n",
    "                res_error2 += error\n",
    "            else:\n",
    "                res_temp, error = extract(v[0])\n",
    "                res_open += res_temp\n",
    "                res_error2 += error\n",
    "e2_go = []\n",
    "for i in res_error2:\n",
    "    if \"GO\" in i:\n",
    "        e2_go.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['regulates(GO:0006366)\\t',\n",
       " '8-sialyltransferase 5\\t\\tgene\\ttaxon:10116\\t20220421\\tGOC\\t\\t',\n",
       " '8-sialyltransferase 5\\t\\tgene\\ttaxon:10116\\t20220421\\tGOC\\t\\t',\n",
       " ' ankyrin repeat and coiled-coil containing 1\\t\\tgene\\ttaxon:10116\\t20180711\\tSynGO\\tpart_of(UBERON:0002421)\\t',\n",
       " ' ankyrin repeat and coiled-coil containing 1\\t\\tgene\\ttaxon:10116\\t20180711\\tSynGO\\tpart_of(UBERON:0002421)\\t',\n",
       " ' ankyrin repeat and coiled-coil containing 1\\t\\tgene\\ttaxon:10116\\t20180711\\tSynGO\\tpart_of(GO:0098978)',\n",
       " ' ankyrin repeat and coiled-coil containing 1\\t\\tgene\\ttaxon:10116\\t20180711\\tSynGO\\tpart_of(GO:0098978)',\n",
       " ' ankyrin repeat and coiled-coil containing 1\\t\\tgene\\ttaxon:10116\\t20180711\\tSynGO\\toccurs_in(GO:0098978)',\n",
       " ' ankyrin repeat and coiled-coil containing 1\\t\\tgene\\ttaxon:10116\\t20180711\\tSynGO\\toccurs_in(GO:0098978)',\n",
       " ' member 2\\t\\tgene\\ttaxon:10116\\t20050726\\tRGD\\tnegatively_regulates(GO:0008283)\\t']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "e2_go[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict = dict()\n",
    "term_set = set()\n",
    "for gene, term in res_open:\n",
    "    if not term in term_set:\n",
    "        term_set.add(term)\n",
    "        res_dict[term]=set([gene])\n",
    "    else:\n",
    "        res_dict[term].add(gene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/github/2023/RatDeconvolution/data/220801_go.owl was loaded.\n",
      "obo.GO_0008150 was determined as root.\n",
      "Graph structure was obtained.\n",
      "String hashtable was created.\n",
      "Indicated depth GO terms : 6712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6712/6712 [00:00<00:00, 8434.59it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of GO terms : 3447\n",
      "No. of genes / term : 7.559036843632144\n"
     ]
    }
   ],
   "source": [
    "res = create_dict(datafile=datafile, gene_dict=res_dict, depth=depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(res, f\"{root}/result/go_rat_depth5.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ssGSEA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import codecs\n",
    "\n",
    "sys.path.append(\"C:/github/enan\")\n",
    "from enan import ssgsea\n",
    "\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotation_sample(df, df_b):\n",
    "    temp = df_b.loc[df.columns.tolist()]\n",
    "    name = temp[\"COMPOUND_NAME\"].tolist()\n",
    "    dose = temp[\"DOSE_LEVEL\"].tolist()\n",
    "    time = temp[\"SACRIFICE_PERIOD\"].tolist()\n",
    "    ind = [f\"{i}_{j}_{k}\" for i, j, k in zip(name, dose, time)]\n",
    "    return ind\n",
    "\n",
    "def load_transcriptome(target_compounds):\n",
    "    # load transcriptome\n",
    "    df_target = pd.read_csv(f\"{root}/data/tggate_transcriptome.csv\",index_col=0)\n",
    "    df_sample = pd.read_csv(f\"{root}/data/tggate_sample_information.csv\",index_col=0)\n",
    "    df_target.columns=[str(i) for i in df_target.columns]\n",
    "    df_sample.index=[str(i) for i in df_sample.index]\n",
    "    df_target.columns = annotation_sample(df_target, df_sample)\n",
    "    target_lst = [\n",
    "        f'{compound}_{conc}_{time}'\n",
    "        for compound in target_compounds \n",
    "        for conc in [\"High\", \"Control\"]\n",
    "        for time in [\"3 hr\", \"6 hr\", \"9 hr\", \"24 hr\"]\n",
    "    ]\n",
    "    df_target=df_target.loc[:,target_lst]\n",
    "    return df_target\n",
    "\n",
    "def calc_ssGSEA(df, depth=5, limit=10):\n",
    "    # load\n",
    "    set_depth = pd.read_pickle(f\"{root}/result/go_rat_depth{str(depth)}.pickle\")\n",
    "    ref = dict()\n",
    "    set_whole=set()\n",
    "    for i in set_depth.keys():\n",
    "        temp = set_depth[i]\n",
    "        if len(temp)>limit-1:\n",
    "            ref[i]=temp\n",
    "            set_whole = set_whole|set_depth[i]\n",
    "    print(f\"ref length: {len(ref)}\")\n",
    "    print(f\"whole genes: {len(set_whole)}\")\n",
    "\n",
    "    dat = ssgsea.ssGSEA()\n",
    "    dat.fit(ref)\n",
    "    dat.set_whole(set_whole)\n",
    "    res = dat.calc(df, method=\"kuiper\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_compounds=[\n",
    "    \"naphthyl isothiocyanate\",\"bromobenzene\",\"simvastatin\",\"enalapril\",\n",
    "    \"gefitinib\",\"metformin\",\"tiopronin\",\"colchicine\",\n",
    "    \"bortezomib\",\"methylene dianiline\",\"galactosamine\",\"thioacetamide\",\n",
    "    \"LPS\",\"cycloheximide\",\"tacrine\",\"nitrofurazone\",\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_transcriptome(target_compounds)\n",
    "col = df.columns.tolist()\n",
    "df.columns=range(len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ref length: 584\n",
      "whole genes: 8098\n",
      "Kuiper method\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 383/383 [07:04<00:00,  1.11s/it]\n"
     ]
    }
   ],
   "source": [
    "res = calc_ssGSEA(df, depth=5)\n",
    "res.columns=col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.to_csv(f\"{root}/result/res_depth5_kuiper.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis/Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import warnings\n",
    "\n",
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import statsmodels.stats.multitest as sm\n",
    "\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams[\"font.size\"] = 16\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_z(res, drugs=list(), ctrl:str=\"Control\"):\n",
    "    df_z = pd.DataFrame(columns=res.columns.tolist())\n",
    "    for drug in drugs:\n",
    "        res_temp = res.loc[res.index.str.contains(drug),:]\n",
    "        res_ctrl = res_temp.loc[res_temp.index.str.contains(ctrl),:]\n",
    "        res_drug = res_temp.loc[~res_temp.index.str.contains(ctrl),:]\n",
    "        \n",
    "        mean = np.nanmean(res_ctrl.values,axis=0)\n",
    "        std = np.nanstd(res_ctrl.values,axis=0)\n",
    "        res_z = pd.DataFrame((res_drug.values - mean)/std)\n",
    "        res_z.index = res_drug.index\n",
    "        res_z.columns = res_drug.columns\n",
    "        res_z = res_z.replace(np.inf,0)\n",
    "        res_z = res_z.replace(-np.inf,0)\n",
    "        res_z = res_z.fillna(0)\n",
    "        if len(res.index)!=0:\n",
    "            df_z = pd.concat([df_z, res_z],axis=0)\n",
    "    return df_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_compounds=[\n",
    "    \"naphthyl isothiocyanate\",\"bromobenzene\",\"simvastatin\",\"enalapril\",\n",
    "    \"gefitinib\",\"metformin\",\"tiopronin\",\"colchicine\",\n",
    "    \"bortezomib\",\"methylene dianiline\",\"galactosamine\",\"thioacetamide\",\n",
    "    \"LPS\",\"cycloheximide\",\"tacrine\",\"nitrofurazone\",\n",
    "    ]\n",
    "\n",
    "clusters = [\n",
    "    [\"naphthyl isothiocyanate\",\"bromobenzene\",\"simvastatin\",\"enalapril\",\"gefitinib\",\"metformin\",\"tiopronin\",],\n",
    "    [\"colchicine\",\"bortezomib\",\"methylene dianiline\",\"galactosamine\",\"thioacetamide\",],\n",
    "    [\"LPS\",\"cycloheximide\"],[\"tacrine\",\"nitrofurazone\"],\n",
    "]\n",
    "depth=5\n",
    "limit=0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
